---
title: "Flu Analysis: Machine Learning Techniques"
author: "Nicholas Mallis"
date: "11/2/2021"
output: html_document
---

This is part of an analysis exercise I did in a Modern Applied Data Analysis course at UGA with Dr. Andreas Handel. We used data on flu patients You can find the full steps to data cleaning on more information [here at the github page](https://github.com/nicholasmallis/NicholasMallis-MADA-analysis3)


# Setting up
```{r message=FALSE, results =FALSE}
#loading packages
library(broom)
library(here) #for data loading/saving
library(tidyverse)
library(recipes)
library(tidymodels)
library(workflowr) 
library(parsnip)
library(rsample)
library(rpart)
library(glmnet)
library(ranger)
library(modeldata)
library(rpart.plot)
library(dials)
library(workflows)
library(vip)
library(glmnet)
library(yardstick)
library(doParallel) # for parallel computing 
```


#Loading Data


```{r message=FALSE, results =FALSE}
#first loading in processed data
data_location <- here::here("files","processeddata.rds")

#load data. 
data <- readRDS(data_location)

#checking
glimpse(data)

```

#Setting up: Random Seed, Data Split, and Cross Validation 


```{r}

set.seed(123)

data_split <- initial_split(data, prop = .7, strata = BodyTemp)

train_data <- training(data_split)
test_data  <- testing(data_split)

folds <- vfold_cv(train_data, v = 5, r=5, strata= "BodyTemp")
folds

```


Creating a Recipe/Workflow
```{r}
#Recipe() has two arguments: a formula and the data
bodytemp_cont_rec <- recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) #adding step_dummy

#Build a model specification using the parsnip package
lm_mod <- linear_reg() %>%
  set_engine("lm") 

#Model workflow pairs a model and recipe together
bodytemp_cont_workflow <- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(bodytemp_cont_rec)


```

#Fitting the Null Model
```{r}

# Creates a simple recipe that fits null model
bodytmp_rec_null <- recipe(BodyTemp ~  1 , data = train_data)

# Set a model as we did in the previous exercise
lr_mod <- 
  linear_reg() %>% 
  set_engine("lm")

bodytmp_wflow_null <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(bodytmp_rec_null)

# Fitting the model
bodytmp_fit_null <- 
  bodytmp_wflow_null %>% 
  fit(data = train_data)

# Extracting Model/Recipes with Parsnip
bodytmp_fit_null %>% 
  extract_fit_parsnip() %>% 
  tidy()


# Obtaining Predictions
predict(bodytmp_fit_null, train_data)

bodytmp_aug_null <- 
  augment(bodytmp_fit_null, train_data)

bodytmp_aug_null %>%
  select(BodyTemp)


# Calculating Root RMSE 
rmse_train <- bodytmp_aug_null %>% 
  rmse(truth = BodyTemp, .pred)

# RMSE 1.21
rmse_train



# Now on Test Data


# Obtaining Predictions
predict(bodytmp_fit_null, test_data)

bodytmp_aug_null <- 
  augment(bodytmp_fit_null, test_data)

bodytmp_aug_null %>%
  select(BodyTemp)


# Calculating Root RMSE 
rmse_test <- bodytmp_aug_null %>% 
  rmse(truth = BodyTemp, .pred)

# RMSE 1.16
rmse_test 
```

###Here we see that the RMSE on train data from the null model is 1.21
```{r}
# RMSE 1.21
rmse_train
```



#Fitting a Tree
```{r}
# TREE 

# model specification
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")    # setting it to regression instead of classification

tune_spec


# tuning grid specification
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)


tree_grid %>% 
  count(tree_depth)


# Tune a workflow() that bundles together a model
# specification and a recipe or model preprocessor.
# Here we use a workflow() with a straightforward formula; 
# if this model required more involved data preprocessing, 
# we could use add_recipe() instead of add_formula().


tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(bodytemp_cont_rec) # using predefined recipe

```



```{r}
# tuning using cross-validation and the tune_grid() function
tree_res <- 
  tree_wf %>% 
  tune_grid(resamples = folds, grid = tree_grid)
```

```{r}
tree_res %>% 
  collect_metrics()

# Once you have done the tuning, you can take a look at some diagnostics
#by sending your object returned from the tune_grid() function to autoplot(). 
#For instance if you tuned the tree and saved the result as tree_tune_res,
#you can run tree_tune_res %>% autoplot(). Depending on the model, the plot
#will be different, but in general it shows you what happened during the tuning process.

#plotting metrics
tree_res %>% autoplot()


# Next, you want to get the model that the tuning process has determined 
# is the best. You can get the best-fit model with select_best() 
# and finalize_workflow() and then do one more fit to the training data with 
# this final workflow using the fit() function. Follow the examples in the tutorial.

# selecting best
best_tree <- tree_res %>%
  select_best(tree_res, metric = "rsq")

best_tree



# finalizing model
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf

# one more fit to the training data with 
# this final workflow using the fit() function

final_fit <- 
  final_wf %>%
  last_fit(data_split) 


# RMSE= 1.23, not much different from the null
final_fit %>%
  collect_metrics()

#Collecting Predictions
tree_pred <- final_fit %>%
  collect_predictions() 

```




```{r}
# Make two plots, one that shows model predictions from the tuned model 
# versus actual outcomes
  
ggplot(data=tree_pred, aes(x=.pred, y=BodyTemp)) + geom_point() + labs(title= "Plot of Model Predictions from Tuned Model vs Actual Outcomes", 
       x= "Model Predictions", y= "Actual Outcomes") 

#calculating residuals
tree_pred$resid <- tree_pred$BodyTemp - tree_pred$.pred 

# one that plots residuals.
# plotting residuals 
ggplot(data=tree_pred, aes(x=.pred , y=resid)) + geom_point() +
  labs(title= "Plot of Model Predictions from Tuned Model vs Actual Outcomes", 
       x= "Model Predictions", y= "Residuals") 



# Look at/print the model performance and compare it with the null model
# (still only on training data). Here, we want the performance of the tuned, 
# best-fitting model on the CV dataset (we are not yet touching the test data). 
# You can get that for instance with the show_best() function, which gives you
# the mean cross-validated performance for the best models. It also shows the
# standard deviation for the performance. Compare that model performance with the null model

```


###Comparing RMSE to Null

The tree model does not perform very well, and the model only predicts a few discrete
outcome values. That’s also noticeable when we compare RMSE for the tree model(1.23) 
and the null model (1.21). They are very similar.

```{r}
# RMSE= 1.23
show_best(final_fit, metric= "rmse")

# Null Model. RMSE 1.21
rmse_train
```


###Tree Plot
```{r}
final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```


### Estimate variable importance based on the model’s structure.
```{r}
library(vip)

final_fit %>% 
  extract_fit_parsnip() %>% 
  vip()
```





## LASSO linear model

Repeating the steps above, now for LASSO.


### LASSO setup


```{r, start-lasso}
#model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) #mixture = 1 means we use the LASSO model
#workflow
lasso_wf <- workflow() %>%
  add_model(lasso_model) %>% 
  add_recipe(bodytemp_cont_rec)
```



### LASSO tuning

```{r, tune-lasso}
#parallel computing
#ncores = 18 #adjust based on your computer
#cl <- makePSOCKcluster(ncores)
#registerDoParallel(cl)
#tuning grid
lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
#tune model
lasso_tune_res <- lasso_wf %>% 
  tune_grid(resamples = folds,
            grid = lasso_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse)
            )
# turn off parallel cluster

```


### LASSO evaluation

```{r}
#see a plot of performance for different tuning parameters
lasso_tune_res %>% autoplot()
```




```{r}
# get the tuned model that performs best 
best_lasso <- lasso_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_lasso_wf <- lasso_wf %>% finalize_workflow(best_lasso)
# fitting best performing model
best_lasso_fit <- best_lasso_wf %>% 
  fit(data = train_data)
lasso_pred <- predict(best_lasso_fit, train_data)
```

Plotting LASSO variables as function of tuning parameter


```{r}
x <- best_lasso_fit$fit$fit$fit
plot(x, "lambda")
```

The higher the regularization, the fewer predictor variables that remain in the model. (Once a coefficient is at 0, the corresponding variable is not in the model anymore).


Below are the variables that are part of the best-fit LASSO.

```{r}
tidy(extract_fit_parsnip(best_lasso_fit)) %>% filter(estimate != 0)
```


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(lasso_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(lasso_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

The diagnostic plots show that this model isn't much better either. We want the points to be along the red lines in each plot. They are not.



Looking at model performance. 

```{r}
lasso_perfomance <- lasso_tune_res %>% show_best(n = 1)
print(lasso_perfomance)
```

The mean RMSE was 1.15, which is still not great.



## Random forest model

Repeating the steps above, now for a random forest.


### Random forest setup


```{r, start-rf}
rf_model <- rand_forest() %>%
  set_args(mtry = tune(),     
    trees = tune(),
    min_n = tune()
  ) %>%
  # select the engine/package that underlies the model
  set_engine("ranger",
             num.threads = 18, #for some reason for RF, we need to set this in the engine too
             importance = "permutation") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("regression")           
```


```{r}
#workflow
rf_wf <- workflow() %>%
  add_model(rf_model) %>% 
  add_recipe(bodytemp_cont_rec)
```


### Random forest tuning

```{r, tune-rf}
#parallel computing
#tuning grid
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
# tune the model, optimizing RMSE
rf_tune_res <- rf_wf %>%
  tune_grid(
            resamples = folds, #CV object
            grid = rf_grid, # grid of values to try
            metrics = metric_set(rmse) 
  )
# turn off parallel cluster
```


### Random forest evaluation

```{r}
#see a plot of performance for different tuning parameters
rf_tune_res %>% autoplot()
```



```{r}
# get the tuned model that performs best 
best_rf <- rf_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_rf_wf <- rf_wf %>% finalize_workflow(best_rf)
# fitting best performing model
best_rf_fit <- best_rf_wf %>% 
  fit(data = train_data)
rf_pred <- predict(best_rf_fit, train_data)
```


Looking at important predictors
```{r}
#pull out the fit object
x <- best_rf_fit$fit$fit$fit
#plot variable importance
vip::vip(x, num_features = 20)
```

Sneeze and Subject fever look like the most important.



Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(rf_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(rf_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```


Looking at model performance. 

```{r}
rf_perfomance <- rf_tune_res %>% show_best(n = 1)
print(rf_perfomance)
```

Based on the diagnostic plots and the model performance, it seems that RF isn't much better.



# Picking a Model 

None of these models are peforming that well, but for the sake of the exercise, we go with the LASSO.


# Final Model Evaluation

We'll now apply the model a single time to the test data.

```{r}
# for reasons that make no sense (likely a bug in tidymodels)
# I need to re-start a parallel cluster here to get the command below to work 
# fit on the training set and evaluate on test set
final_fit <- best_lasso_wf  %>% last_fit(data_split)

```

Let's look at the performance of the final fit, evaluated on the test data.

```{r}
test_performance <- final_fit %>% collect_metrics()
print(test_performance)
```

Looking at RMSE on test and train, we that they are similar, suggesting that we avoided overfitting. 

If we compare the RMSE on the test data to the performance/RMSE of the null model on the test data, we see it's not much better, showing that none of these models are good.


Diagnostic plots for test data.

```{r}
test_predictions <- final_fit %>% collect_predictions()
```


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(test_predictions$.pred,test_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(test_predictions$.pred-test_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```




